---
title: "Practical Machine Learning - Assess Weight Lifting Exercise"
author: "David Doyle"
date: "June 21, 2015"
output: html_document
---

## Summary

This project uses the weight lifting exercise data sourced from a [Human Activity Recognition](http://groupware.les.inf.puc-rio.br/har) project at Pontifícia Universidade Católica do Rio de Janeiro.  See below for the citation.

> Velloso, E.; Bulling, A.; Gellersen, H.; Ugulino, W.; Fuks, H. [Qualitative Activity Recognition of Weight Lifting Exercises](http://groupware.les.inf.puc-rio.br/work.jsf?p1=11201). Proceedings of 4th International Conference in Cooperation with SIGCHI (Augmented Human '13) . Stuttgart, Germany: ACM SIGCHI, 2013.

The purpose of this exercise is to use the collected data to create a model that will predict the correctness of a weight lifting exercise - with A indicating a correctly executed dumbbell curl and B, C, D and E indicating different classes of mistakes.

This paper documents the approach taken to solving the problem.

## Initialization
To speed up processing the *doParallel* library was used to distribute the training across multiple CPU cores. The *caret* and *AppliedPredictiveModelling* libraries are used to provide a standard machine learning API.
```{r}
# Machine Learning Project
# enable multi-core processing
library(doParallel)
cl <- makeCluster(detectCores())
registerDoParallel(cl)

library(AppliedPredictiveModeling)
library(caret)

```


## Loading Training Data

The training and testing datasets are downloaded as CSV files

```{r}
# First download the training / testing dataset if they have 
# not already been downloaded

setwd("C:/Users/david/datasciencecoursera/Practical Machine Learning/Week2/Project")
trainingFile <- "pml-training.csv"
trainingURL <- "https://d396qusza40orc.cloudfront.net/predmachlearn/pml-training.csv"
testFile <- "pml-testing.csv"
testURL <- "https://d396qusza40orc.cloudfront.net/predmachlearn/pml-testing.csv"


if(!file.exists(trainingFile)) {
  download.file(trainingURL, destfile=trainingFile)
}

if(!file.exists(testFile)) {
  download.file(testURL, destfile=testFile)
}

```


## Cleaning The Training Data
Manual examination of the training file revealed that uncaptured data was in one of three different formats:

* blank/empty values
* what looks to be a division by zero error
* values of NA

All three of these were converted to *NA* on loading into a data frame.  Column containing 97% *NA* values were removed to clean the data.  I also removed several fields not relevant to predicting the outcome of an exercise such as timestamps, participant names etc.

The cleaned data frame was then divided into training (25%) and validation (75%) data frames.

```{r}
# read in the file - several values are standardized to NA

trainingData <- read.csv(trainingFile, na.strings = c("#DIV/0!","","NA"))

# remove constant columns not needed for training
trainingData <- subset(trainingData, 
                       select = -c(X,user_name,raw_timestamp_part_1, 
                       raw_timestamp_part_2, cvtd_timestamp, new_window,num_window) )

# get names of columns with a large number of NAs

NAcolsToDrop <- colnames(trainingData)[colSums(is.na(trainingData)) > nrow(trainingData) * 0.97]

trainingData <- trainingData[, !names(trainingData) %in% NAcolsToDrop]

# set a seed value for repeatability
set.seed(3433)
inTrain = createDataPartition(trainingData$classe, p = 0.25, list=FALSE)
training = trainingData[inTrain,]
validation = trainingData[-inTrain,]
```

## Training, Validation & Expected Error
A number of training methods were attempted including glm and rpart with/without principal component analysis (the code is in the .R file but omitted in this document for brevity).  However the estimated accuracy was low with those methods.  The best method attempted was to use random forests.  Once the training was competed it was run against the validation set and the actual results in the validation set compared to the results of the prediction. The matrix below shows an estimated accuracy of 97% with very good sensitivity and specificity values.

```{r}
modelFit <- train( classe ~ ., data=training, method="rf")
testPC1 <- predict(modelFit,newdata=validation)
confusionMatrix(validation$classe, testPC1)
```

Finally, we shut down the parallel processing.

```{r}
stopCluster(cl)
# The stopCluster is necessary to terminate the extra processes
```

## Run Against Test Set

After confirming that the random forests approach looks usable based on the validation set results it was run against the test set of 20 measurements.

The test set was cleaned in a similar manner to the training set, removing the same columns to obtain cleansed data.  The prediction was then run and the tests saved into individual files.  The individual files were then uploaded to Coursera with only one value scoring as incorrect, which is closely aligned with the expected error i.e. 1 out of 20.

```{r}

# read in the file - several values are standardized to NA

testData <- read.csv(testFile, na.strings = c("#DIV/0!","","NA"))


# remove constant columns not needed for training
testData <- subset(testData, 
                       select = -c(X,user_name,raw_timestamp_part_1, 
                                   raw_timestamp_part_2, cvtd_timestamp, new_window,num_window) )

# Remove the same NA containing columns as was done with 
# the training dataset
testData <- testData[, !names(testData) %in% NAcolsToDrop]

testPC2 <- predict(modelFit,newdata=testData)

answers <- as.character(testPC2)

print(answers)

pml_write_files = function(x){
  n = length(x)
  for(i in 1:n){
    filename = paste0("problem_id_",i,".txt")
    write.table(x[i],file=filename,quote=FALSE,row.names=FALSE,col.names=FALSE)
  }
}

pml_write_files(answers)

```

